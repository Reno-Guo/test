import streamlit as st
import pandas as pd
import os
from datetime import datetime
import io
import zipfile
import tempfile
import calendar

def save_df_to_buffer(df: pd.DataFrame) -> io.BytesIO:
    buffer = io.BytesIO()
    df.to_excel(buffer, index=False, engine="openpyxl")
    buffer.seek(0)
    return buffer

def render_app_header(emoji_title: str, subtitle: str):
    st.markdown(f"""
    <div style="background: linear-gradient(135deg, #00a6e4 0%, #0088c2 100%); padding: 2rem; border-radius: 10px; margin-bottom: 2rem; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
        <h2 style="color: white; margin: 0; display: flex; align-items: center;">
            {emoji_title}
        </h2>
        <p style="color: rgba(255,255,255,0.9); margin-top: 0.5rem;">{subtitle}</p>
    </div>
    """, unsafe_allow_html=True)

def csv_to_dataframe(csv_path: str, header_row: int = 0) -> pd.DataFrame:
    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'cp1252']
    for encoding in encodings:
        try:
            df = pd.read_csv(csv_path, encoding=encoding, header=header_row)
            return df
        except (UnicodeDecodeError, pd.errors.ParserError):
            continue
    df = pd.read_csv(csv_path, encoding='utf-8', header=header_row, encoding_errors='ignore')
    return df

def excel_to_dataframe(excel_path: str, header_row: int = 0) -> pd.DataFrame:
    return pd.read_excel(excel_path, header=header_row)

def process_zip_files_with_preview(uploaded_file, header_row: int, file_type: str):
    if uploaded_file is None:
        return pd.DataFrame()
    
    with tempfile.TemporaryDirectory() as temp_dir:
        zip_path = os.path.join(temp_dir, uploaded_file.name)
        with open(zip_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        with zipfile.ZipFile(zip_path, "r") as z:
            z.extractall(temp_dir)
        
        files = [f for f in os.listdir(temp_dir) if f.lower().endswith(('.csv', '.xlsx', '.xls'))]
        if not files:
            st.warning(f"ðŸ“‚ {file_type}åŽ‹ç¼©åŒ…ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡ä»¶")
            return pd.DataFrame()
        
        dfs = []
        for f in files:
            fp = os.path.join(temp_dir, f)
            try:
                if f.lower().endswith('.csv'):
                    df = csv_to_dataframe(fp, header_row=header_row)
                else:
                    df = excel_to_dataframe(fp, header_row=header_row)
                
                with st.expander(f"ðŸ“„ {file_type} - {f} é¢„è§ˆ"):
                    st.write(f"**åˆ—å:** {list(df.columns)}")
                    st.write(f"**å½¢çŠ¶:** {df.shape}")
                    st.dataframe(df.head(3), use_container_width=True)
                dfs.append(df.reset_index(drop=True))
            except Exception as e:
                st.error(f"âŒ å¤„ç† {f} å¤±è´¥: {str(e)[:100]}...")
        
        if not dfs:
            return pd.DataFrame()
        
        result = pd.concat(dfs, ignore_index=True, sort=False)
        return result

def process_zip_files(uploaded_file, header_row: int):
    if uploaded_file is None:
        return pd.DataFrame()
    
    with tempfile.TemporaryDirectory() as temp_dir:
        zip_path = os.path.join(temp_dir, uploaded_file.name)
        with open(zip_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        with zipfile.ZipFile(zip_path, "r") as z:
            z.extractall(temp_dir)
        
        files = [f for f in os.listdir(temp_dir) if f.lower().endswith(('.csv', '.xlsx', '.xls'))]
        if not files:
            return pd.DataFrame()
        
        dfs = []
        for f in files:
            fp = os.path.join(temp_dir, f)
            try:
                if f.lower().endswith('.csv'):
                    df = csv_to_dataframe(fp, header_row=header_row)
                else:
                    df = excel_to_dataframe(fp, header_row=header_row)
                dfs.append(df.reset_index(drop=True))
            except:
                continue
        
        if not dfs:
            return pd.DataFrame()
        
        result = pd.concat(dfs, ignore_index=True, sort=False)
        return result

def parse_month_year_to_yyyy_mm(col_name: str) -> str:
    """å°† 'December 2023' æˆ– 'December-2023' è½¬ä¸º '2023-12'"""
    clean = col_name.replace(',', '').replace('-', ' ').strip()
    parts = clean.split()
    if len(parts) < 2:
        return col_name  # æ— æ³•è§£æžåˆ™åŽŸæ ·è¿”å›ž
    month_name, year_str = parts[0], parts[1]
    try:
        month_num = list(calendar.month_name).index(month_name.capitalize())
        return f"{year_str}-{month_num:02d}"
    except ValueError:
        return col_name  # æ— æ•ˆæœˆä»½ååˆ™åŽŸæ ·è¿”å›ž

def merge_monthly_data(rev_df, units_df, asin_df, month_cols):
    """æŒ‰æœˆä»½åˆ†æ‰¹å¤„ç†æ•°æ®åˆå¹¶ï¼Œå‡å°‘å†…å­˜å ç”¨"""
    desired_order = [
        'Product', 'ASIN', 'Brand', 'Price', 'BSR', 'Number of sellers', 'Fulfillment',
        'FBA fees (USD)', 'Ratings', 'Review count', 'Images', 'Buy Box', 'Category',
        'Subcategory', 'Size tier', 'Dimensions', 'Weight', 'Creation date', 'Variation count',
        'Net price', 'Sales trend (90 days)', 'Price trend (90 days)', 'Best sales period',
        'Sales to reviews', 'Parent ASIN', 'Price per unit', 'Unit count', 'Pack form',
        'Manufacturer', 'Unit Sales', 'Unit Sales Actuals', 'Total Revenue', 'Total Revenue Actuals', 'æ—¶é—´'
    ]
    
    # åˆ†æ‰¹å¤„ç†æœˆä»½æ•°æ®
    batch_size = 3  # æ¯æ¬¡å¤„ç†3ä¸ªæœˆçš„æ•°æ®
    results = []
    
    for i in range(0, len(month_cols), batch_size):
        batch_cols = month_cols[i:i+batch_size]
        batch_results = []
        
        for col in batch_cols:
            # å¤„ç†æ”¶å…¥æ•°æ®
            rev_temp = rev_df[['Product', col]].dropna(subset=[col]).copy()
            rev_temp.columns = ['Product', 'Total Revenue']
            time_val = parse_month_year_to_yyyy_mm(col)
            rev_temp['æ—¶é—´'] = time_val
            
            # å¤„ç†é”€å”®æ•°é‡æ•°æ®
            units_temp = units_df[['Product', col]].dropna(subset=[col]).copy()
            units_temp.columns = ['Product', 'Unit Sales']
            units_temp['æ—¶é—´'] = time_val
            
            # åˆå¹¶æ”¶å…¥å’Œå•ä½æ•°æ®
            combined = rev_temp.merge(units_temp, on=['Product', 'æ—¶é—´'], how='inner')
            
            # ä¸ŽASINè¯¦æƒ…åˆå¹¶
            final_batch = asin_df.merge(combined, left_on='ASIN', right_on='Product', how='inner')
            
            # æ¸…ç† _x / _y åˆ—
            if 'Total Revenue_x' in final_batch.columns and 'Total Revenue_y' in final_batch.columns:
                final_batch['Total Revenue'] = final_batch['Total Revenue_y']
                final_batch = final_batch.drop(columns=['Total Revenue_x', 'Total Revenue_y'])
            elif 'Total Revenue_y' in final_batch.columns:
                final_batch = final_batch.rename(columns={'Total Revenue_y': 'Total Revenue'})
            elif 'Total Revenue_x' in final_batch.columns:
                final_batch = final_batch.rename(columns={'Total Revenue_x': 'Total Revenue'})

            if 'Unit Sales_x' in final_batch.columns and 'Unit Sales_y' in final_batch.columns:
                final_batch['Unit Sales'] = final_batch['Unit Sales_y']
                final_batch = final_batch.drop(columns=['Unit Sales_x', 'Unit Sales_y'])
            elif 'Unit Sales_y' in final_batch.columns:
                final_batch = final_batch.rename(columns={'Unit Sales_y': 'Unit Sales'})
            elif 'Unit Sales_x' in final_batch.columns:
                final_batch = final_batch.rename(columns={'Unit Sales_x': 'Unit Sales'})

            if 'Product_x' in final_batch.columns and 'Product_y' in final_batch.columns:
                final_batch['Product'] = final_batch['Product_x']
                final_batch = final_batch.drop(columns=['Product_x', 'Product_y'])
            elif 'Product_y' in final_batch.columns:
                final_batch = final_batch.rename(columns={'Product_y': 'Product'})
            elif 'Product_x' in final_batch.columns:
                final_batch = final_batch.rename(columns={'Product_x': 'Product'})

            # æŒ‰æŒ‡å®šé¡ºåºé‡æŽ’åˆ—
            existing_cols = [col for col in desired_order if col in final_batch.columns]
            extra_cols = [col for col in final_batch.columns if col not in desired_order]
            final_batch = final_batch[existing_cols + extra_cols]
            
            batch_results.append(final_batch)
        
        # åˆå¹¶å½“å‰æ‰¹æ¬¡ç»“æžœ
        if batch_results:
            batch_concat = pd.concat(batch_results, ignore_index=True)
            results.append(batch_concat)
            # æ¸…ç†å½“å‰æ‰¹æ¬¡çš„ä¸­é—´æ•°æ®
            del batch_results
    
    # æœ€ç»ˆåˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    if results:
        final_result = pd.concat(results, ignore_index=True)
        return final_result
    else:
        return pd.DataFrame()

def sales_data_merge_app():
    render_app_header("ðŸ”— é”€å”®æ•°æ®åˆå¹¶å·¥å…·", "åˆå¹¶Rev.ã€Unitsä¸ŽPrducts")
    
    st.markdown("### ðŸ“¥ ä¸Šä¼ æ•°æ®æ–‡ä»¶")
    col1, col2, col3 = st.columns(3)
    with col1:
        rev_zip = st.file_uploader("Rev. ZIP", type=["zip"], key="rev")
    with col2:
        units_zip = st.file_uploader("Units ZIP", type=["zip"], key="units")
    with col3:
        asin_zip = st.file_uploader("Products ZIP", type=["zip"], key="asin")
    
    st.divider()
    preview_btn = st.button("ðŸ” é¢„è§ˆå„æ–‡ä»¶å†…å®¹", use_container_width=True)
    execute_btn = st.button("ðŸš€ å¼€å§‹åˆå¹¶æ•°æ®", use_container_width=True)
    
    if preview_btn:
        if not all([rev_zip, units_zip, asin_zip]):
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ å…¨éƒ¨ä¸‰ä¸ªæ–‡ä»¶")
            return
        
        with st.spinner("åŠ è½½é¢„è§ˆä¸­..."):
            process_zip_files_with_preview(rev_zip, header_row=1, file_type="Rev.")
            process_zip_files_with_preview(units_zip, header_row=1, file_type="Units")
            process_zip_files_with_preview(asin_zip, header_row=0, file_type="Products")
    
    if execute_btn:
        if not all([rev_zip, units_zip, asin_zip]):
            st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ä¸‰ä¸ªZIPæ–‡ä»¶")
            return
        
        with st.spinner("å¤„ç†æ•°æ®ä¸­..."):
            rev_df = process_zip_files(rev_zip, header_row=1)
            units_df = process_zip_files(units_zip, header_row=1)
            asin_df = process_zip_files(asin_zip, header_row=0)
            
            if rev_df.empty or units_df.empty or asin_df.empty:
                st.error("âŒ æŸä¸ªæ–‡ä»¶åŠ è½½å¤±è´¥")
                return
            
            # èŽ·å–æœˆä»½åˆ—
            month_cols = [col for col in rev_df.columns if col not in ['Product', 'Product Name', 'Brand', 'Total']]
            
            # æŒ‰æœˆä»½åˆ†æ‰¹å¤„ç†æ•°æ®åˆå¹¶
            final = merge_monthly_data(rev_df, units_df, asin_df, month_cols)
            
            if final.empty:
                st.warning("âš ï¸ æ— åŒ¹é…è®°å½•")
                return
            
            # ä¿å­˜ç»“æžœ
            buffer = save_df_to_buffer(final)
            out_name = f"merged_sales_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.xlsx"
            
            st.success(f"âœ… åˆå¹¶å®Œæˆï¼å…± {len(final)} è¡Œæ•°æ®")
            st.dataframe(final.head(10), use_container_width=True)
            
            st.download_button(
                "ðŸ“¥ ä¸‹è½½åˆå¹¶ç»“æžœ",
                data=buffer,
                file_name=out_name,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )

if __name__ == "__main__":
    sales_data_merge_app()
