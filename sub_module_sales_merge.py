import streamlit as st
import pandas as pd
import os
from datetime import datetime
import io
import zipfile
import tempfile
import calendar
from pathlib import Path

# ... (ä¿æŒåŸæœ‰çš„ save_df_to_buffer, render_app_header, csv_to_dataframe, 
# excel_to_dataframe, parse_month_year_to_yyyy_mm å‡½æ•°ä¸å˜)

def extract_and_get_files(uploaded_zip, temp_dir: str):
    """è§£å‹ zip å¹¶è¿”å›æ‰€æœ‰æ•°æ®æ–‡ä»¶è·¯å¾„"""
    zip_path = os.path.join(temp_dir, uploaded_zip.name)
    with open(zip_path, "wb") as f:
        f.write(uploaded_zip.getbuffer())
    
    with zipfile.ZipFile(zip_path, "r") as z:
        z.extractall(temp_dir)
    
    files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) 
             if f.lower().endswith(('.csv', '.xlsx', '.xls'))]
    return files

def read_product_df(file_path: str, header_row: int) -> pd.DataFrame:
    """è¯»å–å•ä¸ªäº§å“æ–‡ä»¶"""
    if file_path.lower().endswith('.csv'):
        return csv_to_dataframe(file_path, header_row)
    else:
        return pd.read_excel(file_path, header=header_row)

def get_month_columns(df: pd.DataFrame) -> list:
    """è·å–æ‰€æœ‰æœˆä»½åˆ—ï¼ˆæ’é™¤å¸¸è§éæœˆä»½åˆ—ï¼‰"""
    exclude = {'Product', 'Product Name', 'Brand', 'Total', 'ASIN'}
    return [col for col in df.columns if col not in exclude]

def process_single_month(rev_files, units_files, asin_df, month_col, temp_dir, idx):
    """å¤„ç†å•ä¸ªæœˆä»½çš„æ•°æ®"""
    # 1. åªè¯»å–å½“å‰æœˆä»½éœ€è¦çš„åˆ—
    rev_cols = ['Product', month_col]
    units_cols = ['Product', month_col]
    
    rev_parts = []
    for fp in rev_files:
        try:
            if fp.lower().endswith('.csv'):
                df = pd.read_csv(fp, usecols=lambda c: c in rev_cols, header=1)
            else:
                df = pd.read_excel(fp, usecols=rev_cols, header=1)
            rev_parts.append(df)
        except:
            continue
    
    if not rev_parts:
        return None
    rev_month = pd.concat(rev_parts, ignore_index=True).dropna(subset=[month_col])
    
    # 2. Units åŒç†
    units_parts = []
    for fp in units_files:
        try:
            if fp.lower().endswith('.csv'):
                df = pd.read_csv(fp, usecols=lambda c: c in units_cols, header=1)
            else:
                df = pd.read_excel(fp, usecols=units_cols, header=1)
            units_parts.append(df)
        except:
            continue
    
    if not units_parts:
        return None
    units_month = pd.concat(units_parts, ignore_index=True).dropna(subset=[month_col])
    
    # 3. è½¬æˆè§„èŒƒæ ¼å¼
    rev_month = rev_month.rename(columns={month_col: 'Total Revenue'})
    rev_month['æ—¶é—´'] = parse_month_year_to_yyyy_mm(month_col)
    
    units_month = units_month.rename(columns={month_col: 'Unit Sales'})
    units_month['æ—¶é—´'] = parse_month_year_to_yyyy_mm(month_col)
    
    # 4. åˆå¹¶ Rev + Units
    combined = rev_month.merge(units_month, on='Product', how='inner')
    
    # 5. åŒ¹é… ASIN ä¿¡æ¯
    result = asin_df.merge(combined, left_on='ASIN', right_on='Product', how='inner')
    
    # 6. ç«‹å³ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆparquet æ›´çœç©ºé—´ä¸”å¿«ï¼‰
    if not result.empty:
        temp_path = os.path.join(temp_dir, f"month_result_{idx:03d}.parquet")
        result.to_parquet(temp_path, index=False, compression='snappy')
        return temp_path
    return None

def sales_data_merge_app():
    render_app_header("ğŸ”— é”€å”®æ•°æ®åˆå¹¶å·¥å…·ï¼ˆåˆ†æœˆä½å†…å­˜ç‰ˆï¼‰", "é€æœˆå¤„ç†ï¼Œå†…å­˜å ç”¨å¤§å¹…é™ä½")
    
    # ... ä¸Šä¼ æ§ä»¶éƒ¨åˆ†ä¿æŒä¸å˜ ...
    
    col1, col2, col3 = st.columns(3)
    with col1:
        rev_zip = st.file_uploader("Rev. ZIP", type=["zip"], key="rev")
    with col2:
        units_zip = st.file_uploader("Units ZIP", type=["zip"], key="units")
    with col3:
        asin_zip = st.file_uploader("Products ZIP", type=["zip"], key="asin")
    
    # ... é¢„è§ˆæŒ‰é’®éƒ¨åˆ†å¯ä¿ç•™æˆ–ç®€åŒ– ...
    
    if st.button("ğŸš€ å¼€å§‹åˆ†æœˆåˆå¹¶ï¼ˆä½å†…å­˜ï¼‰", use_container_width=True):
        if not all([rev_zip, units_zip, asin_zip]):
            st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ä¸‰ä¸ªZIPæ–‡ä»¶")
            return
            
        with st.spinner("æ­£åœ¨åˆ†æœˆå¤„ç†æ•°æ®ï¼ˆå†…å­˜å‹å¥½æ¨¡å¼ï¼‰..."):
            with tempfile.TemporaryDirectory() as temp_dir:
                # 1. è§£å‹æ‰€æœ‰æ–‡ä»¶
                rev_files = extract_and_get_files(rev_zip, temp_dir)
                units_files = extract_and_get_files(units_zip, temp_dir)
                asin_files = extract_and_get_files(asin_zip, temp_dir)
                
                if not (rev_files and units_files and asin_files):
                    st.error("âŒ æŸäº›å‹ç¼©åŒ…ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®æ–‡ä»¶")
                    return
                
                # 2. è¯»å–å®Œæ•´çš„ ASIN è¡¨ï¼ˆä¸€èˆ¬æ¯”è¾ƒå°ï¼‰
                asin_dfs = [read_product_df(fp, header_row=0) for fp in asin_files]
                asin_df = pd.concat(asin_dfs, ignore_index=True).drop_duplicates(subset=['ASIN'])
                
                # 3. è·å–æ‰€æœ‰æœˆä»½ï¼ˆä»¥ Rev çš„åˆ—ä¸ºå‡†ï¼‰
                sample_rev = read_product_df(rev_files[0], header_row=1)
                month_columns = get_month_columns(sample_rev)
                
                if not month_columns:
                    st.error("âŒ æ— æ³•è¯†åˆ«ä»»ä½•æœˆä»½åˆ—")
                    return
                
                st.info(f"æ£€æµ‹åˆ° {len(month_columns)} ä¸ªæœˆä»½ï¼Œå¼€å§‹é€æœˆå¤„ç†...")
                
                temp_files = []
                progress_bar = st.progress(0)
                
                for i, month_col in enumerate(month_columns):
                    temp_file = process_single_month(
                        rev_files, units_files, asin_df, month_col, temp_dir, i
                    )
                    if temp_file:
                        temp_files.append(temp_file)
                    
                    progress_bar.progress((i + 1) / len(month_columns))
                
                if not temp_files:
                    st.error("âŒ æ‰€æœ‰æœˆä»½å¤„ç†åæ— æœ‰æ•ˆæ•°æ®")
                    return
                
                # 4. åˆå¹¶æ‰€æœ‰ä¸´æ—¶ parquet æ–‡ä»¶
                final_parts = [pd.read_parquet(f) for f in temp_files]
                final = pd.concat(final_parts, ignore_index=True)
                
                # 5. åˆ—æ’åºï¼ˆä¿æŒåŸé¡ºåºé€»è¾‘ï¼‰
                desired_order = [...]  # ä½ åŸæ¥çš„ desired_order åˆ—è¡¨
                existing_cols = [col for col in desired_order if col in final.columns]
                extra_cols = [col for col in final.columns if col not in desired_order]
                final = final[existing_cols + extra_cols]
                
                # 6. è¾“å‡ºç»“æœ
                buffer = save_df_to_buffer(final)
                out_name = f"merged_sales_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.xlsx"
                
                st.success(f"âœ… åˆå¹¶å®Œæˆï¼å…± {len(final):,} è¡Œæ•°æ®ï¼ˆ{len(month_columns)} ä¸ªæœˆï¼‰")
                st.dataframe(final.head(10), use_container_width=True)
                
                st.download_button(
                    "ğŸ“¥ ä¸‹è½½åˆå¹¶ç»“æœ",
                    data=buffer,
                    file_name=out_name,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
