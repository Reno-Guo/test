import streamlit as st
import pandas as pd
import os
import re
from datetime import datetime
import io
import zipfile
import tempfile
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
import plotly.express as px
from uuid import uuid4
from typing import Callable, List, Any, Dict

# ä»ä¸»ç¨‹åºå¯¼å…¥å…±äº«å‡½æ•°
def _read_excel_cached(file_or_path, sheet_name=0, engine=None):
    return pd.read_excel(file_or_path, sheet_name=sheet_name, engine=engine)

def unique_tmp_path(suggest_name: str, default_ext: str = ".xlsx") -> str:
    base, ext = os.path.splitext(suggest_name or f"result{default_ext}")
    ext = ext or default_ext
    return os.path.join("/tmp", f"{base}_{st.session_state.SID}_{uuid4().hex[:8]}{ext}")

def save_df_to_buffer(df: pd.DataFrame) -> io.BytesIO:
    buffer = io.BytesIO()
    df.to_excel(buffer, index=False, engine="openpyxl")
    buffer.seek(0)
    return buffer

def render_download_section(
    buffer: io.BytesIO,
    file_name: str,
    mime_type: str,
    download_label: str,
    key_prefix: str,
    has_save: bool = False,
    save_func: Callable[[], None] | None = None,
    save_path: str | None = None,
):
    if has_save:
        col_d, col_s = st.columns(2)
        with col_d:
            st.download_button(
                label=download_label,
                data=buffer,
                file_name=file_name,
                mime=mime_type,
                key=f"{key_prefix}_download",
                use_container_width=True,
            )
        with col_s:
            if st.checkbox("ğŸ’¾ åŒæ—¶ä¿å­˜åˆ° /tmp ç›®å½•", key=f"{key_prefix}_save"):
                if save_func:
                    save_func()
                st.info(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ° {save_path}")
    else:
        st.download_button(
            label=download_label,
            data=buffer,
            file_name=file_name,
            mime=mime_type,
            key=f"{key_prefix}_download",
            use_container_width=True,
        )

def render_app_header(emoji_title: str, subtitle: str):
    st.markdown(f"""
    <div style="background: linear-gradient(135deg, #00a6e4 0%, #0088c2 100%); padding: 2rem; border-radius: 10px; margin-bottom: 2rem; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
        <h2 style="color: white; margin: 0; display: flex; align-items: center;">
            {emoji_title}
        </h2>
        <p style="color: rgba(255,255,255,0.9); margin-top: 0.5rem;">{subtitle}</p>
    </div>
    """, unsafe_allow_html=True)

def get_timestamp() -> str:
    return datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

def read_csv_with_encoding(file_path, **kwargs):
    """å°è¯•å¤šç§ç¼–ç è¯»å–CSVæ–‡ä»¶"""
    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'cp1252']
    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, encoding=encoding, **kwargs)
            return df
        except UnicodeDecodeError:
            continue
        except Exception:
            continue
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç 
    return pd.read_csv(file_path, **kwargs)

def process_zip_files(
    uploaded_file,
    read_cb: Callable[[str], pd.DataFrame | None],
    process_cb: Callable[[pd.DataFrame, str, str], Any],
) -> List[Any]:
    with tempfile.TemporaryDirectory() as temp_dir:
        zip_path = os.path.join(temp_dir, uploaded_file.name)
        with open(zip_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        with zipfile.ZipFile(zip_path, "r") as z:
            z.extractall(temp_dir)
        files = [f for f in os.listdir(temp_dir) if f.lower().endswith((".xlsx", ".xls", ".csv"))]
        if not files:
            st.warning("ğŸ“‚ å‹ç¼©æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä»»ä½• Excel æˆ– CSV æ–‡ä»¶")
            return []
        results = []
        pb = st.progress(0)
        status = st.empty()
        for i, f in enumerate(files):
            status.text(f"æ­£åœ¨å¤„ç†: {f} ({i+1}/{len(files)})")
            fp = os.path.join(temp_dir, f)
            try:
                df = read_cb(fp)
                if df is None:
                    raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
                results.append(process_cb(df, f, temp_dir))
            except Exception as e:
                st.error(f"âŒ å¤„ç†æ–‡ä»¶ {f} å¤±è´¥: {e}")
            pb.progress((i + 1) / len(files))
        status.empty()
        pb.empty()
        return results

def read_month_rev_file(file_path: str) -> pd.DataFrame | None:
    """è¯»å–æœˆåº¦æ”¶å…¥æ–‡ä»¶ï¼Œè¡¨å¤´åœ¨ç¬¬äºŒè¡Œ"""
    try:
        df = read_csv_with_encoding(file_path, header=1)
        return df
    except Exception as e:
        st.error(f"è¯»å–æœˆåº¦æ”¶å…¥æ–‡ä»¶å¤±è´¥: {e}")
        return None

def read_month_units_file(file_path: str) -> pd.DataFrame | None:
    """è¯»å–æœˆåº¦å•ä½æ–‡ä»¶ï¼Œè¡¨å¤´åœ¨ç¬¬äºŒè¡Œ"""
    try:
        df = read_csv_with_encoding(file_path, header=1)
        return df
    except Exception as e:
        st.error(f"è¯»å–æœˆåº¦å•ä½æ–‡ä»¶å¤±è´¥: {e}")
        return None

def read_asin_detail_file(file_path: str) -> pd.DataFrame | None:
    """è¯»å–ASINè¯¦ç»†ä¿¡æ¯æ–‡ä»¶"""
    try:
        df = read_csv_with_encoding(file_path)
        return df
    except Exception as e:
        st.error(f"è¯»å–ASINè¯¦ç»†ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {e}")
        return None

def sales_data_merge_app():
    render_app_header("ğŸ”— é”€å”®æ•°æ®åˆå¹¶å·¥å…·", "åˆå¹¶æœˆåº¦æ”¶å…¥ã€å•ä½æ•°æ®ä¸ASINè¯¦ç»†ä¿¡æ¯")
    
    st.markdown("### ğŸ“¥ ä¸Šä¼ æ•°æ®æ–‡ä»¶")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        rev_zip_file = st.file_uploader("é€‰æ‹©æœˆåº¦æ”¶å…¥ZIPæ–‡ä»¶ (by month rev.)", type=["zip"], key="rev_zip")
    with col2:
        units_zip_file = st.file_uploader("é€‰æ‹©æœˆåº¦å•ä½ZIPæ–‡ä»¶ (by month units)", type=["zip"], key="units_zip")
    with col3:
        asin_zip_file = st.file_uploader("é€‰æ‹©ASINè¯¦ç»†ä¿¡æ¯ZIPæ–‡ä»¶", type=["zip"], key="asin_zip")
    
    st.divider()
    
    col1, col2 = st.columns([2, 1])
    with col1:
        output_filename = st.text_input("è¾“å‡ºæ–‡ä»¶å", "merged_sales_data.xlsx", key="merge_output_filename")
    with col2:
        st.write("")  # ç©ºç™½åˆ—ï¼Œä¿æŒå¯¹é½
        st.write("")  # ç©ºç™½åˆ—ï¼Œä¿æŒå¯¹é½
    
    st.divider()
    
    execute_btn = st.button("ğŸš€ å¼€å§‹åˆå¹¶æ•°æ®", key="merge_execute", use_container_width=True)
    
    if execute_btn:
        if not (rev_zip_file and units_zip_file and asin_zip_file):
            st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ä¸‰ä¸ªZIPæ–‡ä»¶")
            return
        
        with st.spinner("ğŸ”„ æ­£åœ¨å¤„ç†æ•°æ®ï¼Œè¯·ç¨å€™..."):
            # è¯»å–æœˆåº¦æ”¶å…¥æ•°æ®
            rev_results = process_zip_files(rev_zip_file, read_month_rev_file, lambda df, fname, tdir: df)
            if not rev_results:
                st.error("âŒ æ— æ³•è¯»å–æœˆåº¦æ”¶å…¥æ•°æ®")
                return
            # ç¡®ä¿æ‰€æœ‰DataFrameæœ‰ç›¸åŒçš„åˆ—ç»“æ„åå†åˆå¹¶
            if rev_results:
                # è·å–æ‰€æœ‰å¯èƒ½çš„åˆ—å
                all_columns = set()
                for df in rev_results:
                    all_columns.update(df.columns.tolist())
                
                # æ ‡å‡†åŒ–æ‰€æœ‰DataFrameçš„åˆ—
                standardized_rev_results = []
                for df in rev_results:
                    # æ·»åŠ ç¼ºå¤±çš„åˆ—å¹¶å¡«å……å€¼ä¸ºNaN
                    missing_cols = all_columns - set(df.columns)
                    for col in missing_cols:
                        df[col] = pd.NA
                    # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
                    df = df.reindex(columns=sorted(all_columns))
                    standardized_rev_results.append(df)
                
                rev_df = pd.concat(standardized_rev_results, ignore_index=True)
            else:
                st.error("âŒ æ— æ³•è¯»å–æœˆåº¦æ”¶å…¥æ•°æ®")
                return
            
            # è¯»å–æœˆåº¦å•ä½æ•°æ®
            units_results = process_zip_files(units_zip_file, read_month_units_file, lambda df, fname, tdir: df)
            if not units_results:
                st.error("âŒ æ— æ³•è¯»å–æœˆåº¦å•ä½æ•°æ®")
                return
            # ç¡®ä¿æ‰€æœ‰DataFrameæœ‰ç›¸åŒçš„åˆ—ç»“æ„åå†åˆå¹¶
            if units_results:
                # è·å–æ‰€æœ‰å¯èƒ½çš„åˆ—å
                all_columns = set()
                for df in units_results:
                    all_columns.update(df.columns.tolist())
                
                # æ ‡å‡†åŒ–æ‰€æœ‰DataFrameçš„åˆ—
                standardized_units_results = []
                for df in units_results:
                    # æ·»åŠ ç¼ºå¤±çš„åˆ—å¹¶å¡«å……å€¼ä¸ºNaN
                    missing_cols = all_columns - set(df.columns)
                    for col in missing_cols:
                        df[col] = pd.NA
                    # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
                    df = df.reindex(columns=sorted(all_columns))
                    standardized_units_results.append(df)
                
                units_df = pd.concat(standardized_units_results, ignore_index=True)
            else:
                st.error("âŒ æ— æ³•è¯»å–æœˆåº¦å•ä½æ•°æ®")
                return
            
            # è¯»å–ASINè¯¦ç»†ä¿¡æ¯æ•°æ®
            asin_results = process_zip_files(asin_zip_file, read_asin_detail_file, lambda df, fname, tdir: df)
            if not asin_results:
                st.error("âŒ æ— æ³•è¯»å–ASINè¯¦ç»†ä¿¡æ¯æ•°æ®")
                return
            # ç¡®ä¿æ‰€æœ‰DataFrameæœ‰ç›¸åŒçš„åˆ—ç»“æ„åå†åˆå¹¶
            if asin_results:
                # è·å–æ‰€æœ‰å¯èƒ½çš„åˆ—å
                all_columns = set()
                for df in asin_results:
                    all_columns.update(df.columns.tolist())
                
                # æ ‡å‡†åŒ–æ‰€æœ‰DataFrameçš„åˆ—
                standardized_asin_results = []
                for df in asin_results:
                    # æ·»åŠ ç¼ºå¤±çš„åˆ—å¹¶å¡«å……å€¼ä¸ºNaN
                    missing_cols = all_columns - set(df.columns)
                    for col in missing_cols:
                        df[col] = pd.NA
                    # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
                    df = df.reindex(columns=sorted(all_columns))
                    standardized_asin_results.append(df)
                
                asin_df = pd.concat(standardized_asin_results, ignore_index=True)
            else:
                st.error("âŒ æ— æ³•è¯»å–ASINè¯¦ç»†ä¿¡æ¯æ•°æ®")
                return
            
            # è·å–é™¤Product Nameã€Brandã€Totalä¹‹å¤–çš„æœˆä»½åˆ—
            month_cols = [col for col in rev_df.columns if col not in ['Product Name', 'Brand', 'Total'] and col in units_df.columns]
            
            # å¤„ç†æœˆåº¦æ”¶å…¥æ•°æ®ï¼Œå°†å…¶è½¬æ¢ä¸ºé•¿æ ¼å¼
            rev_long_list = []
            for month_col in month_cols:
                if month_col in rev_df.columns:
                    month_data = rev_df[['Product', month_col]].copy()
                    month_data = month_data.dropna(subset=[month_col])  # ç§»é™¤ç©ºå€¼
                    month_data = month_data.rename(columns={month_col: 'Total Revenue'})
                    # è§£ææœˆä»½åˆ—åï¼Œè½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
                    try:
                        # å°è¯•è§£ææœˆä»½æ ¼å¼ï¼Œå¦‚ "Dec-23" -> "2023-12"
                        month_year = datetime.strptime(month_col, '%b-%y')
                        month_str = month_year.strftime('%Y-%m')
                    except:
                        # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨åˆ—åä½œä¸ºæ—¶é—´
                        month_str = month_col
                    month_data['æ—¶é—´'] = month_str
                    rev_long_list.append(month_data)
            
            # åˆå¹¶æ‰€æœ‰æœˆä»½çš„æ”¶å…¥æ•°æ®
            if rev_long_list:
                # ç¡®ä¿æ‰€æœ‰DataFrameæœ‰ç›¸åŒçš„åˆ—ç»“æ„
                all_rev_columns = set()
                for df in rev_long_list:
                    all_rev_columns.update(df.columns.tolist())
                
                standardized_rev_long_list = []
                for df in rev_long_list:
                    # æ·»åŠ ç¼ºå¤±çš„åˆ—å¹¶å¡«å……å€¼ä¸ºNaN
                    missing_cols = all_rev_columns - set(df.columns)
                    for col in missing_cols:
                        df[col] = pd.NA
                    # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
                    df = df.reindex(columns=sorted(all_rev_columns))
                    standardized_rev_long_list.append(df)
                
                rev_long_df = pd.concat(standardized_rev_long_list, ignore_index=True)
            else:
                rev_long_df = pd.DataFrame(columns=['Product', 'Total Revenue', 'æ—¶é—´'])
            
            # å¤„ç†æœˆåº¦å•ä½æ•°æ®ï¼Œå°†å…¶è½¬æ¢ä¸ºé•¿æ ¼å¼
            units_long_list = []
            for month_col in month_cols:
                if month_col in units_df.columns:
                    month_data = units_df[['Product', month_col]].copy()
                    month_data = month_data.dropna(subset=[month_col])  # ç§»é™¤ç©ºå€¼
                    month_data = month_data.rename(columns={month_col: 'Unit Sales'})
                    # è§£ææœˆä»½åˆ—åï¼Œè½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
                    try:
                        month_year = datetime.strptime(month_col, '%b-%y')
                        month_str = month_year.strftime('%Y-%m')
                    except:
                        month_str = month_col
                    month_data['æ—¶é—´'] = month_str
                    units_long_list.append(month_data)
            
            # åˆå¹¶æ‰€æœ‰æœˆä»½çš„å•ä½æ•°æ®
            if units_long_list:
                # ç¡®ä¿æ‰€æœ‰DataFrameæœ‰ç›¸åŒçš„åˆ—ç»“æ„
                all_units_columns = set()
                for df in units_long_list:
                    all_units_columns.update(df.columns.tolist())
                
                standardized_units_long_list = []
                for df in units_long_list:
                    # æ·»åŠ ç¼ºå¤±çš„åˆ—å¹¶å¡«å……å€¼ä¸ºNaN
                    missing_cols = all_units_columns - set(df.columns)
                    for col in missing_cols:
                        df[col] = pd.NA
                    # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
                    df = df.reindex(columns=sorted(all_units_columns))
                    standardized_units_long_list.append(df)
                
                units_long_df = pd.concat(standardized_units_long_list, ignore_index=True)
            else:
                units_long_df = pd.DataFrame(columns=['Product', 'Unit Sales', 'æ—¶é—´'])
            
            # ä¸ºäº†å¾—åˆ°æ‚¨ç¤ºä¾‹ä¸­çš„ç»“æœï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªäº§å“-æœˆä»½ç»„åˆç”Ÿæˆä¸€è¡Œ
            # é¦–å…ˆè·å–æ‰€æœ‰äº§å“-æœˆä»½ç»„åˆ
            if not rev_long_df.empty and not units_long_df.empty:
                # åˆå¹¶æ”¶å…¥å’Œå•ä½æ•°æ®
                combined_data = rev_long_df.merge(
                    units_long_df[['Product', 'Unit Sales', 'æ—¶é—´']], 
                    on=['Product', 'æ—¶é—´'], 
                    how='outer'
                )
            elif not rev_long_df.empty:
                combined_data = rev_long_df.copy()
                combined_data['Unit Sales'] = None
            elif not units_long_df.empty:
                combined_data = units_long_df.copy()
                combined_data['Total Revenue'] = None
            else:
                st.error("âŒ æ²¡æœ‰å¯ç”¨çš„æœˆåº¦æ•°æ®è¿›è¡Œåˆå¹¶")
                return
            
            # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰äº§å“-æ—¶é—´ç»„åˆçš„DataFrame
            product_time_combos = combined_data[['Product', 'æ—¶é—´']].drop_duplicates()
            
            # å¯¹æ¯ä¸ªäº§å“-æ—¶é—´ç»„åˆï¼Œå¤åˆ¶ASINè¯¦ç»†ä¿¡æ¯çš„ä¸€è¡Œ
            expanded_results = []
            
            for _, combo in product_time_combos.iterrows():
                product = combo['Product']
                time_period = combo['æ—¶é—´']
                
                # è·å–è¯¥äº§å“çš„ASINè¯¦ç»†ä¿¡æ¯
                product_details = asin_df[asin_df['Product'] == product].copy()
                
                if not product_details.empty:
                    # ä¸ºè¯¥æ—¶é—´å‘¨æœŸæ·»åŠ æ”¶å…¥å’Œå•ä½æ•°æ®
                    rev_mask = (combined_data['Product'] == product) & (combined_data['æ—¶é—´'] == time_period)
                    rev_values = combined_data.loc[rev_mask, 'Total Revenue']
                    unit_mask = (combined_data['Product'] == product) & (combined_data['æ—¶é—´'] == time_period)
                    unit_values = combined_data.loc[unit_mask, 'Unit Sales']
                    
                    # å¤åˆ¶æ¯ä¸€è¡Œå¹¶æ›´æ–°Total Revenueå’ŒUnit Salesåˆ—
                    for idx, row in product_details.iterrows():
                        new_row = row.copy()
                        if not rev_values.empty and pd.notna(rev_values.iloc[0]):
                            new_row['Total Revenue'] = rev_values.iloc[0]
                        if not unit_values.empty and pd.notna(unit_values.iloc[0]):
                            new_row['Unit Sales'] = unit_values.iloc[0]
                        
                        # æ·»åŠ æ—¶é—´åˆ—
                        new_row['æ—¶é—´'] = time_period
                        expanded_results.append(new_row)
            
            if expanded_results:
                final_result = pd.DataFrame(expanded_results)
            else:
                final_result = asin_df.copy()
                final_result['æ—¶é—´'] = None
            
            # ä¿å­˜ç»“æœ
            buffer = save_df_to_buffer(final_result)
            ts = get_timestamp()
            out_name = f"merged_sales_data_{ts}.xlsx"
            out_path = os.path.join("/tmp", out_name)
            
            st.success(f"âœ… æ•°æ®åˆå¹¶å®Œæˆï¼å…±å¤„ç† {len(final_result)} è¡Œæ•°æ®")
            
            # æ˜¾ç¤ºç»“æœé¢„è§ˆ
            st.markdown("### ğŸ“Š åˆå¹¶ç»“æœé¢„è§ˆ")
            st.dataframe(final_result.head(10), use_container_width=True)
            
            save_func = lambda: final_result.to_excel(out_path, index=False, engine="openpyxl")
            render_download_section(
                buffer,
                out_name,
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                "ğŸ“¥ ä¸‹è½½åˆå¹¶åçš„Excelæ–‡ä»¶",
                "sales_merge",
                has_save=True,
                save_func=save_func,
                save_path=out_path,
            )
