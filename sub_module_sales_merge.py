import streamlit as st
import pandas as pd
import os
import re
from datetime import datetime
import io
import zipfile
import tempfile
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
import plotly.express as px
from uuid import uuid4
from typing import Callable, List, Any, Dict

# ä»ä¸»ç¨‹åºå¯¼å…¥å…±äº«å‡½æ•°
def _read_excel_cached(file_or_path, sheet_name=0, engine=None):
    return pd.read_excel(file_or_path, sheet_name=sheet_name, engine=engine)

def unique_tmp_path(suggest_name: str, default_ext: str = ".xlsx") -> str:
    base, ext = os.path.splitext(suggest_name or f"result{default_ext}")
    ext = ext or default_ext
    return os.path.join("/tmp", f"{base}_{st.session_state.SID}_{uuid4().hex[:8]}{ext}")

def save_df_to_buffer(df: pd.DataFrame) -> io.BytesIO:
    buffer = io.BytesIO()
    df.to_excel(buffer, index=False, engine="openpyxl")
    buffer.seek(0)
    return buffer

def render_download_section(
    buffer: io.BytesIO,
    file_name: str,
    mime_type: str,
    download_label: str,
    key_prefix: str,
    has_save: bool = False,
    save_func: Callable[[], None] | None = None,
    save_path: str | None = None,
):
    if has_save:
        col_d, col_s = st.columns(2)
        with col_d:
            st.download_button(
                label=download_label,
                data=buffer,
                file_name=file_name,
                mime=mime_type,
                key=f"{key_prefix}_download",
                use_container_width=True,
            )
        with col_s:
            if st.checkbox("ğŸ’¾ åŒæ—¶ä¿å­˜åˆ° /tmp ç›®å½•", key=f"{key_prefix}_save"):
                if save_func:
                    save_func()
                st.info(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ° {save_path}")
    else:
        st.download_button(
            label=download_label,
            data=buffer,
            file_name=file_name,
            mime=mime_type,
            key=f"{key_prefix}_download",
            use_container_width=True,
        )

def render_app_header(emoji_title: str, subtitle: str):
    st.markdown(f"""
    <div style="background: linear-gradient(135deg, #00a6e4 0%, #0088c2 100%); padding: 2rem; border-radius: 10px; margin-bottom: 2rem; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
        <h2 style="color: white; margin: 0; display: flex; align-items: center;">
            {emoji_title}
        </h2>
        <p style="color: rgba(255,255,255,0.9); margin-top: 0.5rem;">{subtitle}</p>
    </div>
    """, unsafe_allow_html=True)

def get_timestamp() -> str:
    return datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

def make_column_names_unique(cols):
    """ç¡®ä¿åˆ—åå”¯ä¸€ï¼Œå¯¹é‡å¤çš„åˆ—åæ·»åŠ åç¼€"""
    new_cols = []
    seen = {}
    for col in cols:
        if col in seen:
            seen[col] += 1
            new_cols.append(f"{col}_{seen[col]}")
        else:
            seen[col] = 0
            new_cols.append(col)
    return new_cols

def csv_to_dataframe(csv_path: str, header_row: int = 0) -> pd.DataFrame:
    """å°†CSVæ–‡ä»¶è½¬æ¢ä¸ºDataFrameï¼Œå¤„ç†é‡å¤åˆ—å"""
    # å°è¯•å¤šç§ç¼–ç è¯»å–CSV
    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'cp1252', 'iso-8859-1']
    
    for encoding in encodings:
        try:
            df = pd.read_csv(csv_path, encoding=encoding, header=header_row)
            # ç¡®ä¿åˆ—åå”¯ä¸€
            df.columns = make_column_names_unique(df.columns.tolist())
            return df
        except UnicodeDecodeError:
            continue
        except Exception as e:
            if "encoding" in str(e).lower():
                continue
            else:
                # å¦‚æœä¸æ˜¯ç¼–ç é”™è¯¯ï¼Œåˆ™å¯èƒ½æ˜¯å…¶ä»–é—®é¢˜ï¼Œè®°å½•ä½†ç»§ç»­å°è¯•
                continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç å¹¶å¿½ç•¥é”™è¯¯
    df = pd.read_csv(csv_path, encoding='utf-8', header=header_row, encoding_errors='ignore')
    # ç¡®ä¿åˆ—åå”¯ä¸€
    df.columns = make_column_names_unique(df.columns.tolist())
    return df

def excel_to_dataframe(excel_path: str, header_row: int = 0) -> pd.DataFrame:
    """å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºDataFrameï¼Œå¤„ç†é‡å¤åˆ—å"""
    df = pd.read_excel(excel_path, header=header_row)
    # ç¡®ä¿åˆ—åå”¯ä¸€
    df.columns = make_column_names_unique(df.columns.tolist())
    return df

def process_zip_files_with_preview(
    uploaded_file,
    header_row: int = 0,
    file_type: str = "unknown"
) -> pd.DataFrame:
    """å¤„ç†ZIPæ–‡ä»¶ï¼Œå°†æ‰€æœ‰CSV/XLSXæ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ªDataFrameï¼Œå¹¶æä¾›é¢„è§ˆ"""
    if uploaded_file is None:
        return pd.DataFrame()
    
    with tempfile.TemporaryDirectory() as temp_dir:
        zip_path = os.path.join(temp_dir, uploaded_file.name)
        with open(zip_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        with zipfile.ZipFile(zip_path, "r") as z:
            z.extractall(temp_dir)
        
        # è·å–æ‰€æœ‰CSVå’ŒXLSXæ–‡ä»¶
        files = [f for f in os.listdir(temp_dir) if f.lower().endswith(('.csv', '.xlsx', '.xls'))]
        if not files:
            st.warning(f"ğŸ“‚ {file_type}å‹ç¼©æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä»»ä½• CSV æˆ– Excel æ–‡ä»¶")
            return pd.DataFrame()
        
        dfs = []
        pb = st.progress(0)
        status = st.empty()
        
        for i, f in enumerate(files):
            status.text(f"æ­£åœ¨å¤„ç†: {f} ({i+1}/{len(files)})")
            fp = os.path.join(temp_dir, f)
            
            try:
                if f.lower().endswith('.csv'):
                    # CSVæ–‡ä»¶è½¬æ¢ä¸ºDataFrame
                    df = csv_to_dataframe(fp, header_row=header_row)
                else:
                    # Excelæ–‡ä»¶è½¬æ¢ä¸ºDataFrame
                    df = excel_to_dataframe(fp, header_row=header_row)
                
                # æ˜¾ç¤ºå•ä¸ªæ–‡ä»¶çš„é¢„è§ˆ
                with st.expander(f"ğŸ“„ {file_type} - {f} é¢„è§ˆ"):
                    st.write(f"**åˆ—å:** {list(df.columns)}")
                    st.write(f"**å½¢çŠ¶:** {df.shape}")
                    st.dataframe(df.head(5), use_container_width=True)
                
                dfs.append(df)
            except Exception as e:
                st.error(f"âŒ å¤„ç†æ–‡ä»¶ {f} å¤±è´¥: {e}")
            
            pb.progress((i + 1) / len(files))
        
        status.empty()
        pb.empty()
        
        if dfs:
            # åˆå¹¶æ‰€æœ‰DataFrameï¼Œä½¿ç”¨concatçš„ignore_index=Trueå’Œsort=Falseå‚æ•°
            # ä¸ºäº†é¿å…é‡å¤ç´¢å¼•é—®é¢˜ï¼Œæˆ‘ä»¬å…ˆé‡ç½®æ¯ä¸ªDataFrameçš„ç´¢å¼•
            for df in dfs:
                df.reset_index(drop=True, inplace=True)
            
            # åˆå¹¶DataFrame
            result_df = pd.concat(dfs, ignore_index=True, sort=False)
            return result_df
        else:
            return pd.DataFrame()

def sales_data_merge_app():
    render_app_header("ğŸ”— é”€å”®æ•°æ®åˆå¹¶å·¥å…·", "åˆå¹¶æœˆåº¦æ”¶å…¥ã€å•ä½æ•°æ®ä¸ASINè¯¦ç»†ä¿¡æ¯ï¼ˆå«é¢„è§ˆåŠŸèƒ½ï¼‰")
    
    st.markdown("### ğŸ“¥ ä¸Šä¼ æ•°æ®æ–‡ä»¶")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        rev_zip_file = st.file_uploader("é€‰æ‹©æœˆåº¦æ”¶å…¥ZIPæ–‡ä»¶ (by month rev.)", type=["zip"], key="rev_zip")
    with col2:
        units_zip_file = st.file_uploader("é€‰æ‹©æœˆåº¦å•ä½ZIPæ–‡ä»¶ (by month units)", type=["zip"], key="units_zip")
    with col3:
        asin_zip_file = st.file_uploader("é€‰æ‹©ASINè¯¦ç»†ä¿¡æ¯ZIPæ–‡ä»¶", type=["zip"], key="asin_zip")
    
    st.divider()
    
    col1, col2 = st.columns([2, 1])
    with col1:
        output_filename = st.text_input("è¾“å‡ºæ–‡ä»¶å", "merged_sales_data.xlsx", key="merge_output_filename")
    with col2:
        st.write("")  # ç©ºç™½åˆ—ï¼Œä¿æŒå¯¹é½
        st.write("")  # ç©ºç™½åˆ—ï¼Œä¿æŒå¯¹é½
    
    st.divider()
    
    preview_btn = st.button("ğŸ” é¢„è§ˆæ•°æ®", key="preview", use_container_width=True)
    execute_btn = st.button("ğŸš€ å¼€å§‹åˆå¹¶æ•°æ®", key="merge_execute", use_container_width=True)
    
    if preview_btn:
        if not (rev_zip_file and units_zip_file and asin_zip_file):
            st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ä¸‰ä¸ªZIPæ–‡ä»¶")
            return
        
        with st.spinner("ğŸ”„ æ­£åœ¨åŠ è½½é¢„è§ˆæ•°æ®ï¼Œè¯·ç¨å€™..."):
            # é¢„è§ˆæœˆåº¦æ”¶å…¥æ•°æ® (è¡¨å¤´åœ¨ç¬¬2è¡Œï¼Œå³header=1)
            rev_df = process_zip_files_with_preview(rev_zip_file, header_row=1, file_type="æœˆåº¦æ”¶å…¥")
            if not rev_df.empty:
                st.success(f"âœ… æœˆåº¦æ”¶å…¥æ•°æ®å·²åŠ è½½ï¼Œå…± {len(rev_df)} è¡Œ")
                with st.expander("ğŸ“Š æœˆåº¦æ”¶å…¥æ•´ä½“é¢„è§ˆ"):
                    st.write(f"**åˆ—å:** {list(rev_df.columns)}")
                    st.write(f"**å½¢çŠ¶:** {rev_df.shape}")
                    st.dataframe(rev_df.head(5), use_container_width=True)
            else:
                st.warning("âŒ æ— æ³•åŠ è½½æœˆåº¦æ”¶å…¥æ•°æ®")
            
            # é¢„è§ˆæœˆåº¦å•ä½æ•°æ® (è¡¨å¤´åœ¨ç¬¬2è¡Œï¼Œå³header=1)
            units_df = process_zip_files_with_preview(units_zip_file, header_row=1, file_type="æœˆåº¦å•ä½")
            if not units_df.empty:
                st.success(f"âœ… æœˆåº¦å•ä½æ•°æ®å·²åŠ è½½ï¼Œå…± {len(units_df)} è¡Œ")
                with st.expander("ğŸ“Š æœˆåº¦å•ä½æ•´ä½“é¢„è§ˆ"):
                    st.write(f"**åˆ—å:** {list(units_df.columns)}")
                    st.write(f"**å½¢çŠ¶:** {units_df.shape}")
                    st.dataframe(units_df.head(5), use_container_width=True)
            else:
                st.warning("âŒ æ— æ³•åŠ è½½æœˆåº¦å•ä½æ•°æ®")
            
            # é¢„è§ˆASINè¯¦ç»†ä¿¡æ¯æ•°æ® (è¡¨å¤´åœ¨ç¬¬1è¡Œï¼Œå³header=0)
            asin_df = process_zip_files_with_preview(asin_zip_file, header_row=0, file_type="ASINè¯¦æƒ…")
            if not asin_df.empty:
                st.success(f"âœ… ASINè¯¦ç»†ä¿¡æ¯æ•°æ®å·²åŠ è½½ï¼Œå…± {len(asin_df)} è¡Œ")
                with st.expander("ğŸ“Š ASINè¯¦æƒ…æ•´ä½“é¢„è§ˆ"):
                    st.write(f"**åˆ—å:** {list(asin_df.columns)}")
                    st.write(f"**å½¢çŠ¶:** {asin_df.shape}")
                    st.dataframe(asin_df.head(5), use_container_width=True)
            else:
                st.warning("âŒ æ— æ³•åŠ è½½ASINè¯¦ç»†ä¿¡æ¯æ•°æ®")
    
    if execute_btn:
        if not (rev_zip_file and units_zip_file and asin_zip_file):
            st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ä¸‰ä¸ªZIPæ–‡ä»¶")
            return
        
        with st.spinner("ğŸ”„ æ­£åœ¨å¤„ç†æ•°æ®ï¼Œè¯·ç¨å€™..."):
            # è¯»å–æœˆåº¦æ”¶å…¥æ•°æ® (è¡¨å¤´åœ¨ç¬¬2è¡Œï¼Œå³header=1)
            rev_df = process_zip_files_with_preview(rev_zip_file, header_row=1, file_type="")
            if rev_df.empty:
                st.error("âŒ æ— æ³•è¯»å–æœˆåº¦æ”¶å…¥æ•°æ®")
                return
            
            # è¯»å–æœˆåº¦å•ä½æ•°æ® (è¡¨å¤´åœ¨ç¬¬2è¡Œï¼Œå³header=1)
            units_df = process_zip_files_with_preview(units_zip_file, header_row=1, file_type="")
            if units_df.empty:
                st.error("âŒ æ— æ³•è¯»å–æœˆåº¦å•ä½æ•°æ®")
                return
            
            # è¯»å–ASINè¯¦ç»†ä¿¡æ¯æ•°æ® (è¡¨å¤´åœ¨ç¬¬1è¡Œï¼Œå³header=0)
            asin_df = process_zip_files_with_preview(asin_zip_file, header_row=0, file_type="")
            if asin_df.empty:
                st.error("âŒ æ— æ³•è¯»å–ASINè¯¦ç»†ä¿¡æ¯æ•°æ®")
                return
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ‰€éœ€çš„åˆ—
            if 'Product' not in rev_df.columns:
                st.error(f"âŒ æœˆåº¦æ”¶å…¥æ–‡ä»¶ä¸­ç¼ºå°‘ 'Product' åˆ—ã€‚ç°æœ‰åˆ—: {list(rev_df.columns)}")
                return
            
            if 'Product' not in units_df.columns:
                st.error(f"âŒ æœˆåº¦å•ä½æ–‡ä»¶ä¸­ç¼ºå°‘ 'Product' åˆ—ã€‚ç°æœ‰åˆ—: {list(units_df.columns)}")
                return
            
            if 'ASIN' not in asin_df.columns:
                st.error(f"âŒ ASINè¯¦ç»†ä¿¡æ¯æ–‡ä»¶ä¸­ç¼ºå°‘ 'ASIN' åˆ—ã€‚ç°æœ‰åˆ—: {list(asin_df.columns)}")
                return
            
            # è·å–é™¤Product Nameã€Brandã€Totalä¹‹å¤–çš„æœˆä»½åˆ—
            month_cols = [col for col in rev_df.columns if col not in ['Product Name', 'Brand', 'Total'] and col in units_df.columns]
            
            # å¤„ç†æœˆåº¦æ”¶å…¥æ•°æ®ï¼Œå°†å…¶è½¬æ¢ä¸ºé•¿æ ¼å¼
            rev_long_list = []
            for month_col in month_cols:
                if month_col in rev_df.columns:
                    month_data = rev_df[['Product', month_col]].copy()
                    month_data = month_data.dropna(subset=[month_col])  # ç§»é™¤ç©ºå€¼
                    month_data = month_data.rename(columns={month_col: 'Total Revenue'})
                    # è§£ææœˆä»½åˆ—åï¼Œè½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
                    try:
                        # å°è¯•è§£ææœˆä»½æ ¼å¼ï¼Œå¦‚ "Dec-23" -> "2023-12"
                        month_year = datetime.strptime(month_col, '%b-%y')
                        month_str = month_year.strftime('%Y-%m')
                    except:
                        # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨åˆ—åä½œä¸ºæ—¶é—´
                        month_str = month_col
                    month_data['æ—¶é—´'] = month_str
                    rev_long_list.append(month_data)
            
            # åˆå¹¶æ‰€æœ‰æœˆä»½çš„æ”¶å…¥æ•°æ® - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼åˆå¹¶
            if rev_long_list:
                # é‡ç½®æ¯ä¸ªDataFrameçš„ç´¢å¼•ä»¥é¿å…é‡å¤ç´¢å¼•é”™è¯¯
                for df in rev_long_list:
                    df.reset_index(drop=True, inplace=True)
                rev_long_df = pd.concat(rev_long_list, ignore_index=True, sort=False)
            else:
                rev_long_df = pd.DataFrame(columns=['Product', 'Total Revenue', 'æ—¶é—´'])
            
            # å¤„ç†æœˆåº¦å•ä½æ•°æ®ï¼Œå°†å…¶è½¬æ¢ä¸ºé•¿æ ¼å¼
            units_long_list = []
            for month_col in month_cols:
                if month_col in units_df.columns:
                    month_data = units_df[['Product', month_col]].copy()
                    month_data = month_data.dropna(subset=[month_col])  # ç§»é™¤ç©ºå€¼
                    month_data = month_data.rename(columns={month_col: 'Unit Sales'})
                    # è§£ææœˆä»½åˆ—åï¼Œè½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
                    try:
                        month_year = datetime.strptime(month_col, '%b-%y')
                        month_str = month_year.strftime('%Y-%m')
                    except:
                        month_str = month_col
                    month_data['æ—¶é—´'] = month_str
                    units_long_list.append(month_data)
            
            # åˆå¹¶æ‰€æœ‰æœˆä»½çš„å•ä½æ•°æ®
            if units_long_list:
                # é‡ç½®æ¯ä¸ªDataFrameçš„ç´¢å¼•ä»¥é¿å…é‡å¤ç´¢å¼•é”™è¯¯
                for df in units_long_list:
                    df.reset_index(drop=True, inplace=True)
                units_long_df = pd.concat(units_long_list, ignore_index=True, sort=False)
            else:
                units_long_df = pd.DataFrame(columns=['Product', 'Unit Sales', 'æ—¶é—´'])
            
            # åˆå¹¶æ”¶å…¥å’Œå•ä½æ•°æ®
            if not rev_long_df.empty and not units_long_df.empty:
                combined_data = rev_long_df.merge(
                    units_long_df[['Product', 'Unit Sales', 'æ—¶é—´']], 
                    on=['Product', 'æ—¶é—´'], 
                    how='inner'  # å†…è¿æ¥ï¼Œåªä¿ç•™ä¸¤ä¸ªæ•°æ®æ¡†éƒ½æœ‰çš„è®°å½•
                )
            elif not rev_long_df.empty:
                combined_data = rev_long_df.copy()
                combined_data['Unit Sales'] = None
            elif not units_long_df.empty:
                combined_data = units_long_df.copy()
                combined_data['Total Revenue'] = None
            else:
                st.error("âŒ æ²¡æœ‰å¯ç”¨çš„æœˆåº¦æ•°æ®è¿›è¡Œåˆå¹¶")
                return
            
            # é€šè¿‡revæ–‡ä»¶çš„Productåˆ—å’Œasinè¯¦æƒ…æ–‡ä»¶çš„ASINåˆ—è¿›è¡Œå†…è¿æ¥ï¼ˆåªä¿ç•™åŒ¹é…çš„è®°å½•ï¼‰
            if not combined_data.empty:
                # å°†åˆå¹¶çš„æ•°æ®ä¸ASINè¯¦ç»†ä¿¡æ¯æŒ‰Productå’ŒASINåˆ—è¿›è¡Œå†…è¿æ¥
                final_result = asin_df.merge(
                    combined_data,
                    left_on='ASIN',  # ASINè¯¦ç»†ä¿¡æ¯çš„ASINåˆ—
                    right_on='Product',  # æœˆåº¦æ•°æ®çš„Productåˆ—
                    how='inner'  # å†…è¿æ¥ï¼Œåªä¿ç•™ä¸‰è€…éƒ½åŒ¹é…çš„è®°å½•
                )
                
                # åˆ é™¤é‡å¤çš„Productåˆ—ï¼ˆå› ä¸ºASINå’ŒProductåº”è¯¥æ˜¯åŒä¸€åˆ—ï¼‰
                if 'Product_y' in final_result.columns:
                    final_result = final_result.drop(columns=['Product_y'])
                    final_result = final_result.rename(columns={'Product_x': 'Product'})
                elif 'Product' in final_result.columns and 'ASIN' in final_result.columns:
                    # å¦‚æœåªæœ‰ä¸€è¾¹æœ‰Productåˆ—ï¼Œä¿ç•™ASINä½œä¸ºä¸»é”®
                    pass
            else:
                st.error("âŒ æ²¡æœ‰åŒ¹é…çš„è®°å½•å¯ä»¥åˆå¹¶")
                return
            
            if final_result.empty:
                st.warning("âš ï¸ æ²¡æœ‰ä»»ä½•åŒ¹é…çš„è®°å½•ï¼Œè¯·æ£€æŸ¥Productå’ŒASINåˆ—çš„å€¼æ˜¯å¦å¯¹åº”")
                return
            
            # ä¿å­˜ç»“æœ
            buffer = save_df_to_buffer(final_result)
            ts = get_timestamp()
            out_name = f"merged_sales_data_{ts}.xlsx"
            out_path = os.path.join("/tmp", out_name)
            
            st.success(f"âœ… æ•°æ®åˆå¹¶å®Œæˆï¼å…±å¤„ç† {len(final_result)} è¡Œæ•°æ®")
            
            # æ˜¾ç¤ºç»“æœé¢„è§ˆ
            st.markdown("### ğŸ“Š åˆå¹¶ç»“æœé¢„è§ˆ")
            st.dataframe(final_result.head(10), use_container_width=True)
            
            save_func = lambda: final_result.to_excel(out_path, index=False, engine="openpyxl")
            render_download_section(
                buffer,
                out_name,
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                "ğŸ“¥ ä¸‹è½½åˆå¹¶åçš„Excelæ–‡ä»¶",
                "sales_merge",
                has_save=True,
                save_func=save_func,
                save_path=out_path,
            )
