import streamlit as st
import pandas as pd
import os
from datetime import datetime
import io
import zipfile
import tempfile
import calendar
from pathlib import Path

# =============================================================================
# å·¥å…·å‡½æ•°ï¼ˆä¿æŒåŸæœ‰ï¼Œæœªåšå¤§æ”¹åŠ¨ï¼‰
# =============================================================================

def save_df_to_buffer(df: pd.DataFrame) -> io.BytesIO:
    buffer = io.BytesIO()
    df.to_excel(buffer, index=False, engine="openpyxl")
    buffer.seek(0)
    return buffer


def render_app_header(emoji_title: str, subtitle: str):
    st.markdown(f"""
    <div style="background: linear-gradient(135deg, #00a6e4 0%, #0088c2 100%); 
                padding: 2rem; border-radius: 10px; margin-bottom: 2rem; 
                box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
        <h2 style="color: white; margin: 0; display: flex; align-items: center;">
            {emoji_title}
        </h2>
        <p style="color: rgba(255,255,255,0.9); margin-top: 0.5rem;">{subtitle}</p>
    </div>
    """, unsafe_allow_html=True)


def csv_to_dataframe(csv_path: str, header_row: int = 0) -> pd.DataFrame:
    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'cp1252']
    for encoding in encodings:
        try:
            df = pd.read_csv(csv_path, encoding=encoding, header=header_row)
            return df
        except (UnicodeDecodeError, pd.errors.ParserError):
            continue
    # å…œåº•æ–¹æ¡ˆ
    return pd.read_csv(csv_path, encoding='utf-8', header=header_row, encoding_errors='ignore')


def excel_to_dataframe(excel_path: str, header_row: int = 0) -> pd.DataFrame:
    return pd.read_excel(excel_path, header=header_row)


def parse_month_year_to_yyyy_mm(col_name: str) -> str:
    """å°† 'December 2023' æˆ– 'December-2023' è½¬ä¸º '2023-12'"""
    clean = col_name.replace(',', '').replace('-', ' ').strip()
    parts = clean.split()
    if len(parts) < 2:
        return col_name
    month_name, year_str = parts[0], parts[1]
    try:
        month_num = list(calendar.month_name).index(month_name.capitalize())
        return f"{year_str}-{month_num:02d}"
    except ValueError:
        return col_name


# =============================================================================
# åˆ†æœˆå¤„ç†æ ¸å¿ƒå‡½æ•°
# =============================================================================

def extract_and_get_files(uploaded_zip, temp_dir: str):
    """è§£å‹ zip å¹¶è¿”å›æ‰€æœ‰æ•°æ®æ–‡ä»¶è·¯å¾„"""
    if uploaded_zip is None:
        return []
        
    zip_path = os.path.join(temp_dir, uploaded_zip.name)
    with open(zip_path, "wb") as f:
        f.write(uploaded_zip.getbuffer())
    
    with zipfile.ZipFile(zip_path, "r") as z:
        z.extractall(temp_dir)
    
    files = [
        os.path.join(temp_dir, f) for f in os.listdir(temp_dir)
        if f.lower().endswith(('.csv', '.xlsx', '.xls'))
    ]
    return files


def read_product_df(file_path: str, header_row: int) -> pd.DataFrame:
    """è¯»å–å•ä¸ªäº§å“æ–‡ä»¶"""
    try:
        if file_path.lower().endswith('.csv'):
            return csv_to_dataframe(file_path, header_row)
        else:
            return pd.read_excel(file_path, header=header_row)
    except Exception as e:
        st.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {os.path.basename(file_path)}: {str(e)}")
        return pd.DataFrame()


def get_month_columns(df: pd.DataFrame) -> list[str]:
    """å°è¯•è¯†åˆ«æœˆä»½åˆ—"""
    exclude = {
        'Product', 'Product Name', 'Brand', 'Total', 'ASIN', 'SKU',
        'Category', 'Subcategory', 'Parent ASIN'
    }
    
    candidates = [col for col in df.columns if col not in exclude]
    
    # æ›´æ™ºèƒ½çš„æœˆä»½åˆ—åˆ¤æ–­
    month_like = []
    for col in candidates:
        cleaned = str(col).replace(',', '').replace('-', ' ').strip()
        parts = cleaned.split()
        if len(parts) >= 2:
            # åŒ…å«å¹´ä»½ï¼ˆ2000~2099ï¼‰ä¸”æœ‰æœˆä»½è¯çš„å¯èƒ½æ€§è¾ƒé«˜
            has_year = any(p.isdigit() and 2000 <= int(p) <= 2099 for p in parts)
            if has_year:
                month_like.append(col)
    
    return month_like if month_like else candidates[:24]  # æœ€å¤šå‡è®¾24ä¸ªæœˆ


def get_all_month_columns(rev_files: list[str]) -> list[str]:
    """ä»æ‰€æœ‰ Rev æ–‡ä»¶ä¸­æ”¶é›†å¯èƒ½çš„æœˆä»½åˆ—"""
    all_months = set()
    
    for fp in rev_files:
        try:
            # åªè¯»å–ç¬¬ä¸€è¡Œè·å–åˆ—åï¼Œæ•ˆç‡æœ€é«˜
            if fp.lower().endswith('.csv'):
                df_header = pd.read_csv(fp, nrows=1, header=1)
            else:
                df_header = pd.read_excel(fp, nrows=1, header=1)
                
            months = get_month_columns(df_header)
            all_months.update(months)
        except Exception:
            continue
            
    return sorted(list(all_months))


def process_single_month(rev_files, units_files, asin_df, month_col, temp_dir, idx):
    """å¤„ç†å•ä¸ªæœˆä»½çš„æ•°æ®"""
    rev_cols = ['Product', month_col]
    units_cols = ['Product', month_col]
    
    # Rev
    rev_parts = []
    for fp in rev_files:
        try:
            if fp.lower().endswith('.csv'):
                df = pd.read_csv(fp, usecols=lambda c: c in rev_cols, header=1, low_memory=False)
            else:
                df = pd.read_excel(fp, usecols=lambda c: c in rev_cols, header=1, engine='openpyxl')
            if not df.empty:
                rev_parts.append(df)
        except Exception as e:
            st.warning(f"Rev æ–‡ä»¶è¯»å–æœˆä»½ {month_col} å¤±è´¥: {os.path.basename(fp)} - {str(e)}")
            continue
    
    if not rev_parts:
        return None
    rev_month = pd.concat(rev_parts, ignore_index=True).dropna(subset=[month_col], how='all')
    
    # Units
    units_parts = []
    for fp in units_files:
        try:
            if fp.lower().endswith('.csv'):
                df = pd.read_csv(fp, usecols=lambda c: c in units_cols, header=1, low_memory=False)
            else:
                df = pd.read_excel(fp, usecols=lambda c: c in units_cols, header=1, engine='openpyxl')
            if not df.empty:
                units_parts.append(df)
        except Exception as e:
            st.warning(f"Units æ–‡ä»¶è¯»å–æœˆä»½ {month_col} å¤±è´¥: {os.path.basename(fp)} - {str(e)}")
            continue
    
    if not units_parts:
        return None
    units_month = pd.concat(units_parts, ignore_index=True).dropna(subset=[month_col], how='all')
    
    # æ ¼å¼åŒ–
    rev_month = rev_month.rename(columns={month_col: 'Total Revenue'})
    rev_month['æ—¶é—´'] = parse_month_year_to_yyyy_mm(month_col)
    
    units_month = units_month.rename(columns={month_col: 'Unit Sales'})
    units_month['æ—¶é—´'] = parse_month_year_to_yyyy_mm(month_col)
    
    # åˆå¹¶
    combined = rev_month.merge(units_month, on='Product', how='inner')
    
    if combined.empty:
        return None
        
    # ä¸ ASIN è¡¨åŒ¹é…
    result = asin_df.merge(combined, left_on='ASIN', right_on='Product', how='inner')
    
    if result.empty:
        return None
        
    # ä¿å­˜ä¸´æ—¶ parquet
    temp_path = os.path.join(temp_dir, f"month_result_{idx:03d}.parquet")
    result.to_parquet(temp_path, index=False, compression='snappy')
    return temp_path


# =============================================================================
# ä¸»åº”ç”¨å‡½æ•°
# =============================================================================

def sales_data_merge_app():
    render_app_header(
        "ğŸ”— é”€å”®æ•°æ®åˆå¹¶å·¥å…·"
    )
    
    st.markdown("### ğŸ“¥ ä¸Šä¼ æ•°æ®æ–‡ä»¶ï¼ˆZIP æ ¼å¼ï¼‰")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        rev_zip = st.file_uploader("Rev. ZIPï¼ˆæ”¶å…¥ï¼‰", type=["zip"], key="rev_zip")
    with col2:
        units_zip = st.file_uploader("Units ZIPï¼ˆé”€é‡ï¼‰", type=["zip"], key="units_zip")
    with col3:
        asin_zip = st.file_uploader("Products ZIPï¼ˆäº§å“ä¿¡æ¯ï¼‰", type=["zip"], key="asin_zip")
    
    st.divider()
    
    if st.button("ğŸš€ å¼€å§‹åˆå¹¶", type="primary", use_container_width=True):
        if not all([rev_zip, units_zip, asin_zip]):
            st.error("âš ï¸ è¯·ä¸Šä¼ å…¨éƒ¨ä¸‰ä¸ª ZIP æ–‡ä»¶")
            return
            
        with st.spinner("æ­£åœ¨åˆ†æœˆå¤„ç†æ•°æ®ï¼ˆå†…å­˜å‹å¥½æ¨¡å¼ï¼‰..."):
            with tempfile.TemporaryDirectory() as temp_dir:
                # 1. è§£å‹æ‰€æœ‰æ–‡ä»¶
                rev_files = extract_and_get_files(rev_zip, temp_dir)
                units_files = extract_and_get_files(units_zip, temp_dir)
                asin_files = extract_and_get_files(asin_zip, temp_dir)
                
                if not (rev_files and units_files and asin_files):
                    st.error("âŒ è‡³å°‘æœ‰ä¸€ä¸ªå‹ç¼©åŒ…ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ CSV/XLSX æ–‡ä»¶")
                    return
                
                # 2. è¯»å–å®Œæ•´çš„ ASIN è¡¨ï¼ˆé€šå¸¸è¾ƒå°ï¼‰
                asin_dfs = []
                for fp in asin_files:
                    df = read_product_df(fp, header_row=0)
                    if not df.empty:
                        asin_dfs.append(df)
                
                if not asin_dfs:
                    st.error("âŒ æ— æ³•è¯»å–ä»»ä½• Products/ASIN æ•°æ®")
                    return
                    
                asin_df = pd.concat(asin_dfs, ignore_index=True).drop_duplicates(subset=['ASIN'])
                
                # 3. è·å–æ‰€æœ‰å¯èƒ½çš„æœˆä»½åˆ—ï¼ˆä»æ‰€æœ‰ Rev æ–‡ä»¶ä¸­æ”¶é›†ï¼‰
                month_columns = get_all_month_columns(rev_files)
                
                if not month_columns:
                    st.error("âŒ æ— æ³•è¯†åˆ«ä»»ä½•æœˆä»½åˆ—ï¼ˆè¯·æ£€æŸ¥ Rev æ–‡ä»¶çš„åˆ—åæ ¼å¼ï¼‰")
                    return
                
                st.info(f"å…±æ£€æµ‹åˆ° **{len(month_columns)}** ä¸ªæœˆä»½ï¼Œå¼€å§‹é€æœˆå¤„ç†...")
                
                temp_files = []
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i, month_col in enumerate(month_columns):
                    status_text.text(f"å¤„ç†ä¸­: {month_col} ({i+1}/{len(month_columns)})")
                    
                    temp_file = process_single_month(
                        rev_files, units_files, asin_df, month_col, temp_dir, i
                    )
                    if temp_file:
                        temp_files.append(temp_file)
                    
                    progress_bar.progress((i + 1) / len(month_columns))
                
                status_text.empty()
                
                if not temp_files:
                    st.error("âŒ æ‰€æœ‰æœˆä»½å¤„ç†åæ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆæ•°æ®")
                    return
                
                # 4. åˆå¹¶æ‰€æœ‰æœˆä»½ç»“æœ
                st.info("æ­£åœ¨åˆå¹¶æ‰€æœ‰æœˆä»½ç»“æœ...")
                final_parts = [pd.read_parquet(f) for f in temp_files]
                final = pd.concat(final_parts, ignore_index=True)
                
                # 5. æœŸæœ›çš„åˆ—é¡ºåºï¼ˆè¯·æ ¹æ®ä½ çš„å®é™…ä¸šåŠ¡å­—æ®µè°ƒæ•´ï¼‰
                desired_order = [
                    'Product', 'ASIN', 'Brand', 'Price', 'BSR', 'Number of sellers', 'Fulfillment',
                    'FBA fees (USD)', 'Ratings', 'Review count', 'Images', 'Buy Box', 'Category',
                    'Subcategory', 'Size tier', 'Dimensions', 'Weight', 'Creation date', 'Variation count',
                    'Net price', 'Sales trend (90 days)', 'Price trend (90 days)', 'Best sales period',
                    'Sales to reviews', 'Parent ASIN', 'Price per unit', 'Unit count', 'Pack form',
                    'Manufacturer', 'Unit Sales', 'Unit Sales Actuals', 'Total Revenue',
                    'Total Revenue Actuals', 'æ—¶é—´'
                ]
                
                # æ•´ç†åˆ—é¡ºåºï¼šå…ˆæŒ‰æœŸæœ›é¡ºåºï¼Œå†æ”¾å¤šä½™çš„åˆ—
                existing_cols = [col for col in desired_order if col in final.columns]
                extra_cols = [col for col in final.columns if col not in desired_order]
                final = final[existing_cols + extra_cols]
                
                # 6. è¾“å‡ºç»“æœ
                buffer = save_df_to_buffer(final)
                out_name = f"merged_sales_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.xlsx"
                
                st.success(f"âœ… åˆå¹¶å®Œæˆï¼\n"
                          f"æ€»è¡Œæ•°ï¼š{len(final):,} è¡Œ\n"
                          f"æœˆä»½æ•°ï¼š{len(month_columns)} ä¸ªæœˆ\n"
                          f"å”¯ä¸€ ASINï¼š{final['ASIN'].nunique():,}")
                
                st.dataframe(final.head(10), use_container_width=True)
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½åˆå¹¶ç»“æœï¼ˆExcelï¼‰",
                    data=buffer,
                    file_name=out_name,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
