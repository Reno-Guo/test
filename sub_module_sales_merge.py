import streamlit as st
import pandas as pd
import os
import re
from datetime import datetime
import io
import zipfile
import tempfile
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
import plotly.express as px
from uuid import uuid4
from typing import Callable, List, Any, Dict

# ä»ä¸»ç¨‹åºå¯¼å…¥å…±äº«å‡½æ•°
def _read_excel_cached(file_or_path, sheet_name=0, engine=None):
    return pd.read_excel(file_or_path, sheet_name=sheet_name, engine=engine)

def unique_tmp_path(suggest_name: str, default_ext: str = ".xlsx") -> str:
    base, ext = os.path.splitext(suggest_name or f"result{default_ext}")
    ext = ext or default_ext
    return os.path.join("/tmp", f"{base}_{st.session_state.SID}_{uuid4().hex[:8]}{ext}")

def save_df_to_buffer(df: pd.DataFrame) -> io.BytesIO:
    buffer = io.BytesIO()
    df.to_excel(buffer, index=False, engine="openpyxl")
    buffer.seek(0)
    return buffer

def render_download_section(
    buffer: io.BytesIO,
    file_name: str,
    mime_type: str,
    download_label: str,
    key_prefix: str,
    has_save: bool = False,
    save_func: Callable[[], None] | None = None,
    save_path: str | None = None,
):
    if has_save:
        col_d, col_s = st.columns(2)
        with col_d:
            st.download_button(
                label=download_label,
                data=buffer,
                file_name=file_name,
                mime=mime_type,
                key=f"{key_prefix}_download",
                use_container_width=True,
            )
        with col_s:
            if st.checkbox("ğŸ’¾ åŒæ—¶ä¿å­˜åˆ° /tmp ç›®å½•", key=f"{key_prefix}_save"):
                if save_func:
                    save_func()
                st.info(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ° {save_path}")
    else:
        st.download_button(
            label=download_label,
            data=buffer,
            file_name=file_name,
            mime=mime_type,
            key=f"{key_prefix}_download",
            use_container_width=True,
        )

def render_app_header(emoji_title: str, subtitle: str):
    st.markdown(f"""
    <div style="background: linear-gradient(135deg, #00a6e4 0%, #0088c2 100%); padding: 2rem; border-radius: 10px; margin-bottom: 2rem; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
        <h2 style="color: white; margin: 0; display: flex; align-items: center;">
            {emoji_title}
        </h2>
        <p style="color: rgba(255,255,255,0.9); margin-top: 0.5rem;">{subtitle}</p>
    </div>
    """, unsafe_allow_html=True)

def get_timestamp() -> str:
    return datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

def read_csv_with_encoding(file_path, **kwargs):
    """å°è¯•å¤šç§ç¼–ç è¯»å–CSVæ–‡ä»¶"""
    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'cp1252']
    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, encoding=encoding, **kwargs)
            return df
        except UnicodeDecodeError:
            continue
        except Exception:
            continue
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç 
    return pd.read_csv(file_path, **kwargs)

def process_zip_files(
    uploaded_file,
    read_cb: Callable[[str], pd.DataFrame | None],
    process_cb: Callable[[pd.DataFrame, str, str], Any],
) -> List[Any]:
    with tempfile.TemporaryDirectory() as temp_dir:
        zip_path = os.path.join(temp_dir, uploaded_file.name)
        with open(zip_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        with zipfile.ZipFile(zip_path, "r") as z:
            z.extractall(temp_dir)
        files = [f for f in os.listdir(temp_dir) if f.lower().endswith((".xlsx", ".xls", ".csv"))]
        if not files:
            st.warning("ğŸ“‚ å‹ç¼©æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä»»ä½• Excel æˆ– CSV æ–‡ä»¶")
            return []
        results = []
        pb = st.progress(0)
        status = st.empty()
        for i, f in enumerate(files):
            status.text(f"æ­£åœ¨å¤„ç†: {f} ({i+1}/{len(files)})")
            fp = os.path.join(temp_dir, f)
            try:
                df = read_cb(fp)
                if df is None:
                    raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
                results.append(process_cb(df, f, temp_dir))
            except Exception as e:
                st.error(f"âŒ å¤„ç†æ–‡ä»¶ {f} å¤±è´¥: {e}")
            pb.progress((i + 1) / len(files))
        status.empty()
        pb.empty()
        return results

def read_month_rev_file(file_path: str) -> pd.DataFrame | None:
    """è¯»å–æœˆåº¦æ”¶å…¥æ–‡ä»¶ï¼Œè¡¨å¤´åœ¨ç¬¬äºŒè¡Œ"""
    try:
        df = read_csv_with_encoding(file_path, header=1)
        return df
    except Exception as e:
        st.error(f"è¯»å–æœˆåº¦æ”¶å…¥æ–‡ä»¶å¤±è´¥: {e}")
        return None

def read_month_units_file(file_path: str) -> pd.DataFrame | None:
    """è¯»å–æœˆåº¦å•ä½æ–‡ä»¶ï¼Œè¡¨å¤´åœ¨ç¬¬äºŒè¡Œ"""
    try:
        df = read_csv_with_encoding(file_path, header=1)
        return df
    except Exception as e:
        st.error(f"è¯»å–æœˆåº¦å•ä½æ–‡ä»¶å¤±è´¥: {e}")
        return None

def read_asin_detail_file(file_path: str) -> pd.DataFrame | None:
    """è¯»å–ASINè¯¦ç»†ä¿¡æ¯æ–‡ä»¶"""
    try:
        df = read_csv_with_encoding(file_path)
        return df
    except Exception as e:
        st.error(f"è¯»å–ASINè¯¦ç»†ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {e}")
        return None

def sales_data_merge_app():
    render_app_header("ğŸ”— é”€å”®æ•°æ®åˆå¹¶å·¥å…·", "åˆå¹¶æœˆåº¦æ”¶å…¥ã€å•ä½æ•°æ®ä¸ASINè¯¦ç»†ä¿¡æ¯")
    
    st.markdown("### ğŸ“¥ ä¸Šä¼ æ•°æ®æ–‡ä»¶")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        rev_zip_file = st.file_uploader("é€‰æ‹©æœˆåº¦æ”¶å…¥ZIPæ–‡ä»¶ (by month rev.)", type=["zip"], key="rev_zip")
    with col2:
        units_zip_file = st.file_uploader("é€‰æ‹©æœˆåº¦å•ä½ZIPæ–‡ä»¶ (by month units)", type=["zip"], key="units_zip")
    with col3:
        asin_zip_file = st.file_uploader("é€‰æ‹©ASINè¯¦ç»†ä¿¡æ¯ZIPæ–‡ä»¶", type=["zip"], key="asin_zip")
    
    st.divider()
    
    col1, col2 = st.columns([2, 1])
    with col1:
        output_filename = st.text_input("è¾“å‡ºæ–‡ä»¶å", "merged_sales_data.xlsx", key="merge_output_filename")
    with col2:
        st.write("")  # ç©ºç™½åˆ—ï¼Œä¿æŒå¯¹é½
        st.write("")  # ç©ºç™½åˆ—ï¼Œä¿æŒå¯¹é½
    
    st.divider()
    
    execute_btn = st.button("ğŸš€ å¼€å§‹åˆå¹¶æ•°æ®", key="merge_execute", use_container_width=True)
    
    if execute_btn:
        if not (rev_zip_file and units_zip_file and asin_zip_file):
            st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ä¸‰ä¸ªZIPæ–‡ä»¶")
            return
        
        with st.spinner("ğŸ”„ æ­£åœ¨å¤„ç†æ•°æ®ï¼Œè¯·ç¨å€™..."):
            # è¯»å–æœˆåº¦æ”¶å…¥æ•°æ®
            rev_results = process_zip_files(rev_zip_file, read_month_rev_file, lambda df, fname, tdir: df)
            if not rev_results:
                st.error("âŒ æ— æ³•è¯»å–æœˆåº¦æ”¶å…¥æ•°æ®")
                return
            rev_df = pd.concat(rev_results, ignore_index=True)
            
            # è¯»å–æœˆåº¦å•ä½æ•°æ®
            units_results = process_zip_files(units_zip_file, read_month_units_file, lambda df, fname, tdir: df)
            if not units_results:
                st.error("âŒ æ— æ³•è¯»å–æœˆåº¦å•ä½æ•°æ®")
                return
            units_df = pd.concat(units_results, ignore_index=True)
            
            # è¯»å–ASINè¯¦ç»†ä¿¡æ¯æ•°æ®
            asin_results = process_zip_files(asin_zip_file, read_asin_detail_file, lambda df, fname, tdir: df)
            if not asin_results:
                st.error("âŒ æ— æ³•è¯»å–ASINè¯¦ç»†ä¿¡æ¯æ•°æ®")
                return
            asin_df = pd.concat(asin_results, ignore_index=True)
            
            # è·å–é™¤Product Nameã€Brandã€Totalä¹‹å¤–çš„æœˆä»½åˆ—
            month_cols = [col for col in rev_df.columns if col not in ['Product Name', 'Brand', 'Total']]
            
            # å¤„ç†æœˆåº¦æ”¶å…¥æ•°æ®ï¼Œå°†å…¶è½¬æ¢ä¸ºé•¿æ ¼å¼
            rev_long_list = []
            for month_col in month_cols:
                month_data = rev_df[['Product', month_col]].copy()
                month_data = month_data.dropna(subset=[month_col])  # ç§»é™¤ç©ºå€¼
                month_data = month_data.rename(columns={month_col: 'Total Revenue'})
                # è§£ææœˆä»½åˆ—åï¼Œè½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
                try:
                    # å°è¯•è§£ææœˆä»½æ ¼å¼ï¼Œå¦‚ "Dec-23" -> "2023-12"
                    month_year = datetime.strptime(month_col, '%b-%y')
                    month_str = month_year.strftime('%Y-%m')
                except:
                    # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨åˆ—åä½œä¸ºæ—¶é—´
                    month_str = month_col
                month_data['æ—¶é—´'] = month_str
                rev_long_list.append(month_data)
            
            # åˆå¹¶æ‰€æœ‰æœˆä»½çš„æ”¶å…¥æ•°æ®
            rev_long_df = pd.concat(rev_long_list, ignore_index=True)
            
            # å¤„ç†æœˆåº¦å•ä½æ•°æ®ï¼Œå°†å…¶è½¬æ¢ä¸ºé•¿æ ¼å¼
            units_long_list = []
            for month_col in month_cols:
                if month_col in units_df.columns:
                    month_data = units_df[['Product', month_col]].copy()
                    month_data = month_data.dropna(subset=[month_col])  # ç§»é™¤ç©ºå€¼
                    month_data = month_data.rename(columns={month_col: 'Unit Sales'})
                    # è§£ææœˆä»½åˆ—åï¼Œè½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
                    try:
                        month_year = datetime.strptime(month_col, '%b-%y')
                        month_str = month_year.strftime('%Y-%m')
                    except:
                        month_str = month_col
                    month_data['æ—¶é—´'] = month_str
                    units_long_list.append(month_data)
            
            # åˆå¹¶æ‰€æœ‰æœˆä»½çš„å•ä½æ•°æ®
            units_long_df = pd.concat(units_long_list, ignore_index=True) if units_long_list else pd.DataFrame()
            
            # åˆå¹¶æ”¶å…¥æ•°æ®åˆ°ASINè¯¦ç»†ä¿¡æ¯
            result_df = asin_df.copy()
            
            # æ·»åŠ æ—¶é—´åˆ—ï¼Œä¸ºåç»­åˆå¹¶åšå‡†å¤‡
            result_df['æ—¶é—´'] = None
            
            # ä¸æ”¶å…¥æ•°æ®åˆå¹¶
            if not rev_long_df.empty:
                result_df = result_df.merge(
                    rev_long_df[['Product', 'Total Revenue', 'æ—¶é—´']], 
                    left_on=['Product'], 
                    right_on=['Product'], 
                    how='left'
                )
            
            # ä¸å•ä½æ•°æ®åˆå¹¶
            if not units_long_df.empty:
                # ä¸ºäº†é˜²æ­¢è¦†ç›–ä¹‹å‰çš„åˆå¹¶ç»“æœï¼Œä½¿ç”¨å·¦è¿æ¥å¹¶æ›´æ–°ç‰¹å®šåˆ—
                temp_units = units_long_df[['Product', 'Unit Sales', 'æ—¶é—´']].copy()
                temp_units = temp_units.rename(columns={'Unit Sales': 'temp_Unit_Sales', 'æ—¶é—´': 'temp_æ—¶é—´'})
                
                # åˆå¹¶å•ä½æ•°æ®
                result_df = result_df.merge(
                    temp_units, 
                    left_on=['Product'], 
                    right_on=['Product'], 
                    how='left'
                )
                
                # å°†ä¸´æ—¶çš„Unit Saleså’Œæ—¶é—´æ›´æ–°åˆ°ä¸»DataFrame
                result_df['Unit Sales'] = result_df.get('temp_Unit_Sales', pd.Series([None]*len(result_df)))
                result_df['æ—¶é—´'] = result_df.get('temp_æ—¶é—´', result_df['æ—¶é—´'])
                
                # åˆ é™¤ä¸´æ—¶åˆ—
                result_df = result_df.drop(columns=['temp_Unit_Sales', 'temp_æ—¶é—´'], errors='ignore')
            
            # ä¸ºäº†å®ç°æ‚¨è¦æ±‚çš„æ•ˆæœï¼ˆæ¯è¡Œä¸€ä¸ªæœˆä»½ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªäº§å“-æœˆä»½ç»„åˆåˆ›å»ºå•ç‹¬çš„è¡Œ
            # é¦–å…ˆè·å–æ‰€æœ‰äº§å“å’Œæœˆä»½çš„ç»„åˆ
            if not rev_long_df.empty and not units_long_df.empty:
                # åˆå¹¶æ”¶å…¥å’Œå•ä½æ•°æ®
                combined_data = rev_long_df.merge(
                    units_long_df[['Product', 'Unit Sales', 'æ—¶é—´']], 
                    on=['Product', 'æ—¶é—´'], 
                    how='outer'
                )
            elif not rev_long_df.empty:
                combined_data = rev_long_df.copy()
                combined_data['Unit Sales'] = None
            elif not units_long_df.empty:
                combined_data = units_long_df.copy()
                combined_data['Total Revenue'] = None
            else:
                st.error("âŒ æ²¡æœ‰å¯ç”¨çš„æœˆåº¦æ•°æ®è¿›è¡Œåˆå¹¶")
                return
            
            # ä¸ASINè¯¦ç»†ä¿¡æ¯åˆå¹¶
            final_result = asin_df[['Product', 'ASIN']].merge(combined_data, on='Product', how='left')
            final_result = final_result.merge(asin_df.drop(columns=['Product', 'ASIN']), left_on='Product', right_on=asin_df.columns[0], how='left')
            
            # ä¸ºäº†å¾—åˆ°æ‚¨ç¤ºä¾‹ä¸­çš„ç»“æœï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªäº§å“-æœˆä»½ç»„åˆç”Ÿæˆä¸€è¡Œ
            # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰äº§å“-æœˆä»½ç»„åˆçš„DataFrame
            if not combined_data.empty:
                # è·å–æ‰€æœ‰å”¯ä¸€çš„äº§å“å’Œæ—¶é—´ç»„åˆ
                product_time_combos = combined_data[['Product', 'æ—¶é—´']].drop_duplicates()
                
                # å¯¹æ¯ä¸ªäº§å“-æœˆä»½ç»„åˆï¼Œå¤åˆ¶ASINè¯¦ç»†ä¿¡æ¯çš„ä¸€è¡Œ
                expanded_results = []
                
                for _, combo in product_time_combos.iterrows():
                    product = combo['Product']
                    time_period = combo['æ—¶é—´']
                    
                    # è·å–è¯¥äº§å“çš„ASINè¯¦ç»†ä¿¡æ¯
                    product_details = asin_df[asin_df['Product'] == product].copy()
                    
                    if not product_details.empty:
                        # ä¸ºè¯¥æ—¶é—´å‘¨æœŸæ·»åŠ æ”¶å…¥å’Œå•ä½æ•°æ®
                        rev_value = combined_data[
                            (combined_data['Product'] == product) & 
                            (combined_data['æ—¶é—´'] == time_period)
                        ]['Total Revenue'].values
                        
                        unit_value = combined_data[
                            (combined_data['Product'] == product) & 
                            (combined_data['æ—¶é—´'] == time_period)
                        ]['Unit Sales'].values
                        
                        # æ›´æ–°Total Revenueå’ŒUnit Salesåˆ—
                        if len(rev_value) > 0:
                            product_details['Total Revenue'] = rev_value[0]
                        if len(unit_value) > 0:
                            product_details['Unit Sales'] = unit_value[0]
                        
                        # æ·»åŠ æ—¶é—´åˆ—
                        product_details['æ—¶é—´'] = time_period
                        
                        expanded_results.append(product_details)
                
                if expanded_results:
                    final_result = pd.concat(expanded_results, ignore_index=True)
                else:
                    final_result = asin_df.copy()
                    final_result['æ—¶é—´'] = None
            else:
                final_result = asin_df.copy()
                final_result['æ—¶é—´'] = None
            
            # ä¿å­˜ç»“æœ
            buffer = save_df_to_buffer(final_result)
            ts = get_timestamp()
            out_name = f"merged_sales_data_{ts}.xlsx"
            out_path = os.path.join("/tmp", out_name)
            
            st.success(f"âœ… æ•°æ®åˆå¹¶å®Œæˆï¼å…±å¤„ç† {len(final_result)} è¡Œæ•°æ®")
            
            # æ˜¾ç¤ºç»“æœé¢„è§ˆ
            st.markdown("### ğŸ“Š åˆå¹¶ç»“æœé¢„è§ˆ")
            st.dataframe(final_result.head(10), use_container_width=True)
            
            save_func = lambda: final_result.to_excel(out_path, index=False, engine="openpyxl")
            render_download_section(
                buffer,
                out_name,
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                "ğŸ“¥ ä¸‹è½½åˆå¹¶åçš„Excelæ–‡ä»¶",
                "sales_merge",
                has_save=True,
                save_func=save_func,
                save_path=out_path,
            )
